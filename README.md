
# Using Local LLMs with LangChain

Running popular models locally, whether on local machine or Google Colab.


## Run 

Clone the project

```bash
  git clone https://github.com/harrietfiagbor/Local-LLM.git
```

Go to the project directory

```bash
  cd Local-LLM
```

Install dependencies

```bash
  pip install -r requirements.txt
```

Start the LLM inference

```bash
  python src/llm_inference.py
```


